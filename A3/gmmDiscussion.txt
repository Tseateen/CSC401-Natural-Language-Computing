Change on num of speaker, other default
Speaker:0 Accuracy: 1.0
Speaker:1 Accuracy: 1.0
Speaker:3 Accuracy: 1.0
Speaker:5 Accuracy: 1.0
Speaker:10 Accuracy: 1.0


Change on max training iterations maxIter, other default
maxIter:1 Accuracy: 0.96875
maxIter:5 Accuracy: 1.0
maxIter:10 Accuracy: 1.0
maxIter:14 Accuracy: 1.0
maxIter:20 Accuracy: 1.0


Change on num of components M, other default
M:1 Accuracy: 0.96875
M:3 Accuracy: 1.0
M:8 Accuracy: 1.0
M:13 Accuracy: 1.0
M:18 Accuracy: 1.0

How might you improve the classification accuracy of the Gaussian mixtures, without adding more training data?
Seem only changing in accuracy happened in num of components or iteration times decrease to 1, so maybe increase components will improve it without adding more training data.

When would your classifier decide that a given test utterance comes from none of the trained speaker models, and how would your classifier come to this decision?
My classifier decide it by the likelihood. By finding the best likelihood it will pick got the speaker from trained part. Which means there is a very very low possibility that a given test utterance comes from none of the trained speaker model.

Can you think of some alternative methods for doing speaker identification that donâ€™t use Gaussian mixtures?
Hidden Markov Models(HMM)